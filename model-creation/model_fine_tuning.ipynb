{"cells":[{"cell_type":"markdown","metadata":{"id":"RIe2x8C3TRd-"},"source":["This notebook is for fine-tuning the chosen small summarization model to tailor it to our specific purpose (using curated dataset with pairs of (10 reviews, 1 summary))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iph4S9PMTkwu"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjMupdBAGmz5"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n","from datasets import load_dataset\n","import numpy as np\n","import evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"web6ivTMV3dY"},"outputs":[],"source":["dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/review-summarizer/inferenced_summaries.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cj4Cx1fbjzrV"},"outputs":[],"source":["# splitting dataset into train and test sets\n","dataset = dataset.train_test_split(test_size=0.2, seed=42)\n","\n","train_dataset = dataset[\"train\"]\n","test_dataset = dataset[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYk4vl9vkDIU"},"outputs":[],"source":["# splitting the train set into validation set\n","split_train = train_dataset.train_test_split(test_size=0.2, seed=42)\n","\n","train_dataset = split_train[\"train\"]\n","validation_dataset = split_train[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d63mvuMydbZr"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"GouthamVignesh/falcon-arxiv-long-summary-1B\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"Top 10 Reviews\"], padding=\"max_length\", truncation=True)\n","\n","# tokenize each set (into batches)\n","tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n","tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScP2AJfVWTJc"},"outputs":[],"source":["# Breakdown of dataset for training, validation, testing: 64% train, 16% validation, 20% testing\n","\n","small_train_dataset = tokenized_train_dataset.shuffle(seed=42).select(range(1000))\n","small_eval_dataset = tokenized_test_dataset.shuffle(seed=42).select(range(1000))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOmIPR1qig-V"},"outputs":[],"source":["model = AutoModelForSeq2SeqLM.from_pretrained(\"GouthamVignesh/falcon-arxiv-long-summary-1B\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwU1VC69m2oY"},"outputs":[],"source":["# use eval strategy to monitor in fine-tuning\n","training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82ZVl8q3ne4R"},"outputs":[],"source":["# TODO: look into ROUGE for summarization metric\n","metric = evaluate.load(\"accuracy\")\n","\n","# evaluation metric function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEuu-mQNoRn4"},"outputs":[],"source":["# create trainer object\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_eval_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNCFkWYtO24CO+w8iZ6zEwj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
